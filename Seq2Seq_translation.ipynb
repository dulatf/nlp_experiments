{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq translation",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dulatf/nlp_experiments/blob/master/Seq2Seq_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zodPJNT3TsLv",
        "colab_type": "code",
        "outputId": "4094e0d7-b0bf-4f1c-ebce-20401fc79eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# make sure we have the alpha version of tensorflow 2.0 ready to go in this\n",
        "# Collab instance\n",
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 332.1MB 36kB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 26.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 419kB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 49.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "08ShAZk6iMcH",
        "colab_type": "code",
        "outputId": "d0c0930b-f766-4fc3-fba5-e97513377802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "cell_type": "code",
      "source": [
        "# manually download the translation data file\n",
        "!curl -O http://www.manythings.org/anki/deu-eng.zip\n",
        "!mkdir deu-eng\n",
        "%cd /content/deu-eng\n",
        "!unzip ../deu-eng.zip\n",
        "%cd /content"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4375k  100 4375k    0     0  9532k      0 --:--:-- --:--:-- --:--:-- 9532k\n",
            "/content/deu-eng\n",
            "Archive:  ../deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ml_aL6AEeDa1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf # tensorflow of course\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "import io # io to read in our file\n",
        "import re # we use regexp to replace unwated characters in the text\n",
        "import unicodedata # input text might be unicode, we use this to convert to ascii\n",
        "from sklearn.model_selection import train_test_split # we use this to split into training and test\n",
        "import time\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAjTDKZnh4Xy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# helper function to convert unicode characters to ascii, replaces ü with u etc\n",
        "def unicode_to_ascii(w):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
        "# convert the incoming text to ascii, surround punctuation marks with spaces,\n",
        "# remove multiple spaces and surround sentences with <start> <end> markers\n",
        "def process_text(text):\n",
        "    w = unicode_to_ascii(text.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n",
        "    w = w.rstrip().strip()\n",
        "    return '<start> ' + w + ' <end>'\n",
        "# input and target language sentences are separated by a tab, split and process\n",
        "def load_dataset(path, num_examples=None):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    if num_examples == None:\n",
        "        print(\"Loading {} examples from the dataset.\".format(len(lines)))\n",
        "    return zip(*[[process_text(w) for w in l.split('\\t')] for l in lines[:num_examples]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VQhjzvCwrOJz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we use Keras' preprocessing library to embed the text\n",
        "# no magic is happening here, this just tokenizes the text by splitting on spaces\n",
        "# then it creates a dictionary by counting word frequencies\n",
        "# afterwards the words are mapped to integers (ordered by frequency) that we can\n",
        "# then use in our model\n",
        "def tokenize(text):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "    return sequences, tokenizer\n",
        "\n",
        "# load the dataset and tokenize\n",
        "def prepare_dataset(path, num_examples=None):\n",
        "    target_lang, input_lang = load_dataset(path, num_examples)\n",
        "    input_tensor, input_lang_tokenizer = tokenize(input_lang)\n",
        "    target_tensor, target_lang_tokenizer = tokenize(target_lang)\n",
        "    \n",
        "    return input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer\n",
        "\n",
        "# helper function to check length of elements in tensor (sentences)\n",
        "def max_length(tensor):\n",
        "    return max([len(t) for t in tensor])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-J3HebZrvpko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer = prepare_dataset('/content/deu-eng/deu.txt',10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7WHIUvJVeyF",
        "colab_type": "code",
        "outputId": "3298d290-319a-4957-c99b-4220b8533ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "max_length_target, max_length_input = max_length(target_tensor), max_length(input_tensor)\n",
        "print('Max length input: {}\\nMax length target: {}\\n'.format(max_length_input, max_length_target))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length input: 14\n",
            "Max length target: 9\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iYA5gzClWriD",
        "colab_type": "code",
        "outputId": "bc616287-e5ee-4ba5-eca2-44534ccaffe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "# split into training and test set\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print('Input train #: {}\\nTarget train #: {}\\nInput val #: {}\\nTarget val #: {}\\n'.format(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input train #: 8000\n",
            "Target train #: 8000\n",
            "Input val #: 2000\n",
            "Target val #: 2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZSEzvvSKWx4v",
        "colab_type": "code",
        "outputId": "4c0f6318-88bb-4454-cad9-69dabc155441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "cell_type": "code",
      "source": [
        "# look up and print the actual words for a sequence of embedding integers\n",
        "def wordify(tokenizer, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            print('{} ----> {}\\n'.format(t, tokenizer.index_word[t]))\n",
        "wordify(input_lang_tokenizer, input_tensor_train[0])\n",
        "wordify(target_lang_tokenizer, target_tensor_train[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 ----> <start>\n",
            "\n",
            "29 ----> wer\n",
            "\n",
            "17 ----> hat\n",
            "\n",
            "166 ----> nichts\n",
            "\n",
            "22 ----> zu\n",
            "\n",
            "84 ----> tun\n",
            "\n",
            "8 ----> ?\n",
            "\n",
            "2 ----> <end>\n",
            "\n",
            "1 ----> <start>\n",
            "\n",
            "30 ----> who\n",
            "\n",
            "9 ----> s\n",
            "\n",
            "45 ----> not\n",
            "\n",
            "69 ----> busy\n",
            "\n",
            "7 ----> ?\n",
            "\n",
            "2 ----> <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HnPONjyHbaE2",
        "colab_type": "code",
        "outputId": "7ad496d0-aa1a-4bf8-856b-59c14e776d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# next we create a tf.Dataset so tensorflow can consume our prepared data\n",
        "BATCH_SIZE = 16\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(len(input_tensor_train))\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "vocab_input_size = len(input_lang_tokenizer.word_index)+1\n",
        "vocab_target_size = len(target_lang_tokenizer.word_index)+1\n",
        "EMBEDDING_DIM = 256\n",
        "ENCODER_UNITS = 1024\n",
        "vocab_input_size, vocab_target_size"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3519, 2191)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "NL_XohxRcrmp",
        "colab_type": "code",
        "outputId": "3cf8eb25-2944-403d-a5b9-9ad779fb8a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "a, b = next(iter(dataset))\n",
        "a.shape, b.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 14]), TensorShape([16, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "ho0u2-uHc5lg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# at this point our dataset is ready to be consumed by our model ... which we have to build next\n",
        "# first we start with the encoder part,\n",
        "# this is a set of GRUs that encode the input sequence "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n3u4Jlk2-1pY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqQe8u4DG_Zg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_input_size, EMBEDDING_DIM, ENCODER_UNITS, BATCH_SIZE)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_input_batch, sample_target_batch = next(iter(dataset))\n",
        "sample_output, sample_hidden = encoder(sample_input_batch, sample_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hwJOkxXbHUW6",
        "colab_type": "code",
        "outputId": "1c4b0b82-82cd-4818-e89e-547adacf910c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sample_input_batch.shape, sample_output.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 14]), TensorShape([16, 14, 1024]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "xM0_FP2qHZx6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttentionModel(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(AttentionModel,self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "    def call(self, query, values):\n",
        "        hidden_expanded = tf.expand_dims(query, 1) # turn (a,b) tensor into (a,1,b)\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_expanded)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6didJgfnEidi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "attention_layer = AttentionModel(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "drqv00YOEyKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0c9ee13-0d49-41c5-a16d-6356c601c4f9"
      },
      "cell_type": "code",
      "source": [
        "attention_result.shape, attention_weights.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 1024]), TensorShape([16, 14, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "F-WzV_5fE_V2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = AttentionModel(self.dec_units)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vOLFLFLKls-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDSUiMx4NXmx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_input_size, EMBEDDING_DIM, ENCODER_UNITS, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aev4lvuZRDa6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample_decoder_output, _, _  = decoder(tf.random.uniform((BATCH_SIZE,1)), sample_hidden, sample_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aQvsnNwdRPMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b5cabbd-679e-4f4c-8b3c-0144060b762c"
      },
      "cell_type": "code",
      "source": [
        "sample_decoder_output.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 3519])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "J0s69cG8RSiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c53781e2-3f19-47f6-a7ec-51fba6047dd8"
      },
      "cell_type": "code",
      "source": [
        "tf.math.logical_not(tf.math.equal(tf.constant([0,1,0]),0))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=614, shape=(3,), dtype=bool, numpy=array([False,  True, False])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "6SJsZ9zmRjmT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# now we can define an optimizer and the loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "def loss_func(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    lossy = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=lossy.dtype)\n",
        "    lossy *= mask\n",
        "    return tf.reduce_mean(lossy)\n",
        "CHECKPOINT_DIR = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(CHECKPOINT_DIR,'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pCeeo7lPUO4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# now we define the training loop\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']]*BATCH_SIZE,1)\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input,\n",
        "                                                 dec_hidden,\n",
        "                                                 enc_output)\n",
        "            loss += loss_func(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    \n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients,variables))\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tx1il5tMVamG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1446
        },
        "outputId": "88de789a-07c1-4109-d5ea-7e5d223dd096"
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch+1, total_loss/steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time()-start))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.0538\n",
            "Epoch 1 Batch 100 Loss 0.0562\n",
            "Epoch 1 Batch 200 Loss 0.0498\n",
            "Epoch 1 Batch 300 Loss 0.0449\n",
            "Epoch 1 Batch 400 Loss 0.0879\n",
            "Epoch 1 Loss 0.0737\n",
            "Time taken for 1 epoch 26.391298055648804 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0558\n",
            "Epoch 2 Batch 100 Loss 0.0598\n",
            "Epoch 2 Batch 200 Loss 0.0613\n",
            "Epoch 2 Batch 300 Loss 0.0442\n",
            "Epoch 2 Batch 400 Loss 0.0175\n",
            "Epoch 2 Loss 0.0601\n",
            "Time taken for 1 epoch 26.60215711593628 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0229\n",
            "Epoch 3 Batch 100 Loss 0.0354\n",
            "Epoch 3 Batch 200 Loss 0.0746\n",
            "Epoch 3 Batch 300 Loss 0.0518\n",
            "Epoch 3 Batch 400 Loss 0.0284\n",
            "Epoch 3 Loss 0.0552\n",
            "Time taken for 1 epoch 26.02177143096924 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0202\n",
            "Epoch 4 Batch 100 Loss 0.0349\n",
            "Epoch 4 Batch 200 Loss 0.0112\n",
            "Epoch 4 Batch 300 Loss 0.1128\n",
            "Epoch 4 Batch 400 Loss 0.0499\n",
            "Epoch 4 Loss 0.0521\n",
            "Time taken for 1 epoch 26.89323115348816 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0300\n",
            "Epoch 5 Batch 100 Loss 0.0280\n",
            "Epoch 5 Batch 200 Loss 0.0615\n",
            "Epoch 5 Batch 300 Loss 0.0288\n",
            "Epoch 5 Batch 400 Loss 0.0624\n",
            "Epoch 5 Loss 0.0537\n",
            "Time taken for 1 epoch 26.266663074493408 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0384\n",
            "Epoch 6 Batch 100 Loss 0.0725\n",
            "Epoch 6 Batch 200 Loss 0.0261\n",
            "Epoch 6 Batch 300 Loss 0.0747\n",
            "Epoch 6 Batch 400 Loss 0.0336\n",
            "Epoch 6 Loss 0.0514\n",
            "Time taken for 1 epoch 26.60835838317871 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0092\n",
            "Epoch 7 Batch 100 Loss 0.1027\n",
            "Epoch 7 Batch 200 Loss 0.0231\n",
            "Epoch 7 Batch 300 Loss 0.0732\n",
            "Epoch 7 Batch 400 Loss 0.0469\n",
            "Epoch 7 Loss 0.0490\n",
            "Time taken for 1 epoch 26.50061297416687 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0308\n",
            "Epoch 8 Batch 100 Loss 0.0490\n",
            "Epoch 8 Batch 200 Loss 0.0246\n",
            "Epoch 8 Batch 300 Loss 0.0971\n",
            "Epoch 8 Batch 400 Loss 0.0442\n",
            "Epoch 8 Loss 0.0417\n",
            "Time taken for 1 epoch 26.422825574874878 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0310\n",
            "Epoch 9 Batch 100 Loss 0.0365\n",
            "Epoch 9 Batch 200 Loss 0.0155\n",
            "Epoch 9 Batch 300 Loss 0.1093\n",
            "Epoch 9 Batch 400 Loss 0.0687\n",
            "Epoch 9 Loss 0.0395\n",
            "Time taken for 1 epoch 25.94796347618103 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0364\n",
            "Epoch 10 Batch 100 Loss 0.0481\n",
            "Epoch 10 Batch 200 Loss 0.0193\n",
            "Epoch 10 Batch 300 Loss 0.0856\n",
            "Epoch 10 Batch 400 Loss 0.0461\n",
            "Epoch 10 Loss 0.0432\n",
            "Time taken for 1 epoch 27.104912281036377 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8jRh5UIVcEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
        "    sentence = process_text(sentence)\n",
        "    \n",
        "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_input,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "    \n",
        "    hidden = [tf.zeros((1, ENCODER_UNITS))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n",
        "    for t in range(max_length_target):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += target_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "        if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "giDRqhxTeork",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(1,1,1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KuJ-kThog-Zs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    \n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    attention_plot = attention_plot[:len(result.split(' ')),\n",
        "                                    :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Vh6zUfahao7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "861cca39-a806-485a-ed43-32eeaffacddb"
      },
      "cell_type": "code",
      "source": [
        "translate(u'wir lachen')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> wir lachen <end>\n",
            "Predicted translation: we re losing . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAJwCAYAAAAOZ/9HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "I2rkAhuihfeo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1XMyDbckTWR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hM7BXCEYkr2i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Jf48b96kt9z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}