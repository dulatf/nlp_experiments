{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/dulatf/.julia/compiled/v1.1/corpus.ji for corpus [top-level]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "push!(LOAD_PATH, \".\")\n",
    "using Revise\n",
    "using corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tags from tags-universal.txt\n"
     ]
    }
   ],
   "source": [
    "tags = corpus.load_tags(\"tags-universal.txt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from brown-universal.txt\n"
     ]
    }
   ],
   "source": [
    "sentence_dict = corpus.load_corpus(\"brown-universal.txt\", tags);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences = train_test_split(collect(values(sentence_dict)),.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51606, 5734)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(train_sentences), length(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words=unique_words(train_sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=make_tag_frequencies(train_sentences, tags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Most frequent class baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "most_common_tag = find_most_common_tag(freq);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9571373060961977\n",
      "Test accuracy: 0.9320721309220931\n"
     ]
    }
   ],
   "source": [
    "evs=[evaluate_sentence(sen, x->most_common_tag[x]) for sen in train_sentences];\n",
    "println(\"Training accuracy: \", sum(map(last,evs))/sum(map(first,evs)))\n",
    "evs=[evaluate_sentence(sen, x->most_common_tag[x]) for sen in test_sentences];\n",
    "println(\"Test accuracy: \", sum(map(last,evs))/sum(map(first, evs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "source": [
    "# Hidden Markov model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden variables are the tags and the observations are the actual words.\n",
    "First we need tag unigram and bigram counts, from them we can define transition probabilities to go from one tag to another.\n",
    "See [here](https://web.stanford.edu/~jurafsky/slp3/8.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Array{Int64,1}:\n",
       " 132919\n",
       "  75432\n",
       " 130453\n",
       "  50612\n",
       "  34343\n",
       " 123483\n",
       " 248106\n",
       "  13390\n",
       "  44553\n",
       "  26899\n",
       " 164619\n",
       "   1241\n",
       "      0\n",
       "      0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_counts = corpus.unigram_counts(train_sentences, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigram_counts = corpus.bigram_counts(train_sentences, tags);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs, emission_probs = corpus.hmm_parameters(train_sentences, tags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have < s> and < /s> tags now, we can check that the probability to leave any given\n",
    "state is 1, except for the < /s> state. In other words \\sum_{i} P(s_j,s_i) = 1 for any j.\n",
    "The converse is of course not true, the probability to enter any given state is in (0,1) generally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Array{Real,1}:\n",
       " 1.0               \n",
       " 1.0               \n",
       " 1.0               \n",
       " 1.0               \n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 1.0000000000000002\n",
       " 1.0000000000000002\n",
       " 0.9999999999999999\n",
       " 1.0               \n",
       " 1.0               \n",
       " 1.0               \n",
       " 1.0               \n",
       " 0                 "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum([get(transition_probs,(j,i), 0) for i in 1:length(tags)]) for j in 1:length(tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Array{Tuple{String,Real},1}:\n",
       " (\".\", 0)                    \n",
       " (\"ADJ\", 0)                  \n",
       " (\"ADP\", 0)                  \n",
       " (\"ADV\", 0)                  \n",
       " (\"CONJ\", 0)                 \n",
       " (\"DET\", 0.45751237012382273)\n",
       " (\"NOUN\", 0)                 \n",
       " (\"NUM\", 0)                  \n",
       " (\"PRON\", 0)                 \n",
       " (\"PRT\", 0)                  \n",
       " (\"VERB\", 0)                 \n",
       " (\"X\", 0.0024174053182917004)\n",
       " (\"<s>\", 0)                  \n",
       " (\"</s>\", 0)                 "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(zip(tags,[get(emission_probs, (\"the\", i), 0) for i in 1:length(tags)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = Dict{Tuple{Int, Int}, Float64}()\n",
    "ep = Dict{Tuple{String, Int}, Float64}()\n",
    "states = [1, 2, 3]\n",
    "initial = [0.3, 0.4, 0.3]\n",
    "obs = [\"A\", \"B\", \"C\", \"C\", \"D\", \"B\"]\n",
    "tp[(1,1)] = 0.2\n",
    "tp[(1,2)] = 0.8\n",
    "tp[(1,3)] = 0.0\n",
    "tp[(2,1)] = 0.4\n",
    "tp[(2,2)] = 0.2\n",
    "tp[(2,3)] = 0.4\n",
    "tp[(3,1)] = 0.4\n",
    "tp[(3,2)] = 0.2\n",
    "tp[(3,3)] = 0.4\n",
    "ep[(\"A\",1)] = 0.1\n",
    "ep[(\"B\",1)] = 0.2\n",
    "ep[(\"C\",1)] = 0.6\n",
    "ep[(\"D\",1)] = 0.1\n",
    "ep[(\"A\",2)] = 0.3\n",
    "ep[(\"B\",2)] = 0.2\n",
    "ep[(\"C\",2)] = 0.3\n",
    "ep[(\"D\",2)] = 0.2\n",
    "ep[(\"A\",3)] = 0.0\n",
    "ep[(\"B\",3)] = 0.5\n",
    "ep[(\"C\",3)] = 0.25\n",
    "ep[(\"D\",3)] = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 3"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.viterbi(tp,ep,states, initial,obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state=[get(transition_probs, (tag_index(tags,\"<s>\"),i),0) for i in 1:length(tags)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = corpus.viterbi(transition_probs, emission_probs, tags, initial_state, map(first, train_sentences[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 6, 7, 3, 11, 6, 7, 7, 6, 7, 7, 11, 11, 1]\n",
      "[10, 11, 6, 7, 3, 11, 6, 7, 7, 6, 7, 7, 11, 11, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15-element Array{Tuple{String,Int64},1}:\n",
       " (\"To\", 10)       \n",
       " (\"minimize\", 11) \n",
       " (\"the\", 6)       \n",
       " (\"chances\", 7)   \n",
       " (\"of\", 3)        \n",
       " (\"repeating\", 11)\n",
       " (\"the\", 6)       \n",
       " (\"Balafrej\", 7)  \n",
       " (\"debacle\", 7)   \n",
       " (\"the\", 6)       \n",
       " (\"Ibrahim\", 7)   \n",
       " (\"government\", 7)\n",
       " (\"was\", 11)      \n",
       " (\"formed\", 11)   \n",
       " (\".\", 1)         "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(map(last,train_sentences[2]))\n",
    "println(corpus.viterbi(transition_probs, emission_probs, tags,\n",
    "        initial_state, map(first, train_sentences[2]));)\n",
    "train_sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emission_probability (generic function with 1 method)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function emission_probability(word :: String, tag :: Int)\n",
    "    lambda = 1.0\n",
    "    get(emission_probs, (word,tag), 0.0) * lambda + (1.0 - lambda)*1.0/length(train_words)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17966"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(twrongs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 97.54237369150614%\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "twrongs=[]\n",
    "for sen in train_sentences\n",
    "    pred = corpus.viterbi(transition_probs, emission_probability, tags,\n",
    "        initial_state, map(first, sen))\n",
    "    total_count += length(sen)\n",
    "    for i in 1:length(sen)\n",
    "        if pred[i] == sen[i][2]\n",
    "            correct_count += 1\n",
    "        else\n",
    "            push!(twrongs,sen)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "println(\"Training set accuracy: \", 100.0 * correct_count/total_count,\"%\")\n",
    "twrongs=unique(twrongs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 76.51769119869378%\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "wrongs=[]\n",
    "for sen in test_sentences\n",
    "    pred = corpus.viterbi(transition_probs, emission_probability, tags,\n",
    "        initial_state, map(first, sen))\n",
    "    total_count += length(sen)\n",
    "    for i in 1:length(sen)\n",
    "        if pred[i] == sen[i][2]\n",
    "            correct_count += 1\n",
    "        else\n",
    "            push!(wrongs, sen)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "println(\"Test set accuracy: \", 100.0 * correct_count/total_count,\"%\")\n",
    "wrongs = unique(wrongs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwrongs=[]\n",
    "for sen in wrongs\n",
    "    words=map(first,sen);\n",
    "    truth=map(x->tags[x],map(last, sen));\n",
    "    pred=map(x->tags[x],corpus.viterbi(transition_probs,\n",
    "        (w,s)->get(emission_probs, (w,s), 1/length(train_words)), tags,\n",
    "        initial_state, map(first, sen)));\n",
    "    c = 0\n",
    "    for (t,p) in zip(truth, pred)\n",
    "        if t != p\n",
    "            c+=1\n",
    "        end\n",
    "    end\n",
    "    if c > 10\n",
    "        push!(cwrongs, sen)\n",
    "    end\n",
    "end\n",
    "length(cwrongs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m                Word\tTruth\tPred\u001b[39m\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m              Robert\tNOUN\tNOUN\n",
      "\u001b[39m                  O.\tNOUN\tNOUN\n",
      "\u001b[31m             Spurdle\tNOUN\tVERB\u001b[39m\n",
      "\u001b[39m                  is\tVERB\tVERB\n",
      "\u001b[39m            chairman\tNOUN\tNOUN\n",
      "\u001b[39m                  of\tADP\tADP\n",
      "\u001b[39m                 the\tDET\tDET\n",
      "\u001b[39m           committee\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m               which\tDET\tDET\n",
      "\u001b[31m            includes\tVERB\tADJ\u001b[39m\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m               James\tNOUN\tNOUN\n",
      "\u001b[39m                  A.\tNOUN\tNOUN\n",
      "\u001b[31m               Moody\tNOUN\t.\u001b[39m\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m               Frank\tNOUN\tNOUN\n",
      "\u001b[39m                  C.\tNOUN\tNOUN\n",
      "\u001b[39m           Wilkinson\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[31m               Ethel\tNOUN\tADP\u001b[39m\n",
      "\u001b[39m               Coles\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m              Harold\tNOUN\tNOUN\n",
      "\u001b[39m                  G.\tNOUN\tNOUN\n",
      "\u001b[39m                Lacy\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m              Albert\tNOUN\tNOUN\n",
      "\u001b[39m                  W.\tNOUN\tNOUN\n",
      "\u001b[39m               Terry\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m               Henry\tNOUN\tNOUN\n",
      "\u001b[39m                  M.\tNOUN\tNOUN\n",
      "\u001b[31m              Chance\tNOUN\t.\u001b[39m\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[31m                  2d\tADJ\tNOUN\u001b[39m\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m              Robert\tNOUN\tNOUN\n",
      "\u001b[39m                  O.\tNOUN\tNOUN\n",
      "\u001b[31m             Spurdle\tNOUN\t.\u001b[39m\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                 Jr.\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[31m            Harcourt\tNOUN\tADP\u001b[39m\n",
      "\u001b[39m                  N.\tNOUN\tNOUN\n",
      "\u001b[31m             Trimble\tNOUN\t.\u001b[39m\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                 Jr.\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m                John\tNOUN\tNOUN\n",
      "\u001b[39m                  A.\tNOUN\tNOUN\n",
      "\u001b[39m              Moller\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m              Robert\tNOUN\tNOUN\n",
      "\u001b[39m             Zeising\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m             William\tNOUN\tNOUN\n",
      "\u001b[39m                  G.\tNOUN\tNOUN\n",
      "\u001b[39m             Kilhour\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m              Hughes\tNOUN\tNOUN\n",
      "\u001b[39m            Cauffman\tNOUN\tNOUN\n",
      "\u001b[39m                   ,\t.\t.\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[39m                John\tNOUN\tNOUN\n",
      "\u001b[39m                  L.\tNOUN\tNOUN\n",
      "\u001b[31m            Baringer\tNOUN\t.\u001b[39m\n",
      "\u001b[39m                 and\tCONJ\tCONJ\n",
      "\u001b[39m                Mrs.\tNOUN\tNOUN\n",
      "\u001b[31m               Clyde\tNOUN\tADP\u001b[39m\n",
      "\u001b[39m              Newman\tNOUN\tNOUN\n",
      "\u001b[39m                   .\t.\t.\n"
     ]
    }
   ],
   "source": [
    "sen=cwrongs[12]\n",
    "words=map(first,sen);\n",
    "truth=map(x->tags[x],map(last, sen));\n",
    "pts, prob, vmat, bmat = corpus.viterbi(transition_probs,\n",
    "        (w,s)->get(emission_probs, (w,s), 1/length(train_words)),\n",
    "        tags, initial_state, map(first, sen),true)\n",
    "pred=map(x->tags[x],pts);\n",
    "printstyled(lpad(\"Word\",20,\" \"),\"\\tTruth\\tPred\\n\",color=:yellow)\n",
    "for (w,t,p) in zip(words,truth,pred)\n",
    "    printstyled(lpad(w,20,\" \"),\"\\t\",t,\"\\t\",p,\"\\n\",color=if(t==p)\n",
    "            :default\n",
    "        else\n",
    "            :red\n",
    "            end)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Array{Real,1}:\n",
       " 0                    \n",
       " 0                    \n",
       " 0                    \n",
       " 0                    \n",
       " 0                    \n",
       " 0                    \n",
       " 0                    \n",
       " 0                    \n",
       " 0                    \n",
       " 0                    \n",
       " 1.8223898820913746e-5\n",
       " 0                    \n",
       " 0                    \n",
       " 0                    "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[get(emission_probs,(\"tiled\",i),0) for i in 1:length(tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
